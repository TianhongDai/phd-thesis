\chapter{Interpretability of DRL}
\label{ch:drl_interp}
\section{Introduction}
In previous chapters, we have discussed several methods to improve sample efficiency in deep reinforcement learning (DRL), especially in complex robot manipulation tasks with continuous control. However, all of the above experiments are carried out in a simulated environment. Training becomes much more problematic when applying DRL to real-world applications. For example, in a robot manipulation task, which requires an agent to pick up a cube and place it in a target position, computer vision techniques, such as object detection, pose estimation, or semantic segmentation, are used to estimate the position and orientation of the cube. Then, at each timestep, the current information of the cube and the proprioceptive data of the robot arm are provided to the RL agent to select an action. 

To reduce \td{real-world training times}, a number of end-to-end approaches~\cite{gu2017deep,levine2016end,zhu2017target} have been proposed in recent years, which enable the agent to infer the policy according to raw observations (e.g., images) directly. However, such an end-to-end training setup would require the agent to collect samples for a long time in a real-world environment to update the policy network, which may increase the risk of robot/hardware failure and thus elevate the cost of training significantly. An emerging solution is to have the agent trained in a simulated environment before transferring the policy to the real world. In this period, the agent also needs to overcome the ``reality gap"~\cite{jakobi1995noise} between the simulated environment and the real-world environment, and domain randomisation (DR)~\cite{andrychowicz2018learning,james2017transferring,peng2018sim,sadeghi2017cad2rl,tobin2017domain} is a widely used approach for this purpose. When using DR in training, the elements of the simulated environment are varied to some extent, from the visual appearance to the environment dynamics (see Figure~\ref{fig:dr_example}), in order to improve the policy's robustness to various changes in the environment.
\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/chapter6/domain_random.png}
  \caption{Examples of domain randomisation (visual appearances) in the simulated Fetch (top) and Jaco (bottom) robots environments.}
  \label{fig:dr_example}
\end{figure}

However, such an end-to-end training paradigm, which could involve inferring actions based on image inputs, makes the theoretical analysis of both the environment and the agents that act within it practically infeasible. Specifically, deep neural networks (NNs) trained with DRL form a direct ``black box" mapping from observation to action, presenting the challenge of understanding learned control strategies. If we would like to deploy such policies---particularly on robots in the real world---we would also want to be able to \emph{explain} them~\cite{arrieta2020explainable}. An interpretation of the inference of a model can be used not only as a way of explaining the behaviour of the model, but can also be used to characterise other properties, such as safety, fairness, and reliability ~\cite{doshi2017towards}. Although there are many existing approaches to explain NNs~\cite{guidotti2018survey}, each of these methods is subjective to varying degrees.
\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/chapter6/strategies.png}
  \caption{The range of strategies an agent may learn to use when trained to reach a red target, which we split into three components. Firstly, the agent must visually localise the target---which can be accomplished by detecting red, spherical objects, or simply anything with a significant red component. Secondly, the agent can use vision and/or proprioception to guide its arm to the target. Finally, the agent may accomplish the task with varying levels of robustness to changes in the rest of the visual scene. Using a range of analyses, we show that agents trained to accomplish the same task learn different subsets of these strategies, depending on their training conditions.}
  \label{fig:strategies}
\end{figure}

In this chapter, we aim to understand \td{how DRL agents make decision in the environment} and understand how agents trained with DR behave differently from agents trained without. To this end, we train DRL agents under a range of different settings and use relative differences to better interpret their policies. In contrast to most prior work examining DRL policies~\cite{atrey2020exploratory,greydanus2018visualizing, olson2021counterfactual,puri2020explain,rupprecht2020finding,such2018atari,zahavy2016graying}, we not only provide explanations for how policies react to parts of states/individual states, but use our array of experiments to characterise their \emph{global} strategies (see Figure~\ref{fig:strategies}). \td{Through these experiments and analysis, we are able to address following questions:
\begin{itemize}
    \item What is the role of DR on layer learning?
    \item To what extent is visual inputs rather than proprioceptive inputs used by the agent?
    \item How robust are agents to unseen scenarios?
    \item How redundant is the network in its representation? 
\end{itemize}
}
%For instance, we examine the relative importance of different input modalities, and whether agents need their memory to complete tasks. 

In particular, we train different robots with vision-based inputs to perform the task of reaching for a (red) target, and then use different techniques to understand what features of the environment the policies react to. Under certain training settings, the agent may localise the target by simply looking for anything red, and it is only under more challenging training conditions that the agent utilises both colour and shape. Clearly, the latter strategy better represents what we want the agent to learn, while the former is merely a ``shortcut'' \cite{geirhos2020shortcut}.

To be noticed, the structure in this chapter is organised in a different way from the previous ones. In Section~\ref{ch6:sec_interp}, we first introduce the interpretability techniques used in this work. In Section~\ref{ch6:exp}, we detail our customised simulated environments, the network structure of DRL agents, training details, and the settings of test scenarios. Subsequently, in Section~\ref{ch6:results}, we conduct an exhaustive analysis on the trained models using a broad suite of interpretability techniques. Finally, in Section~\ref{ch6:discussion}, we discuss our findings and provide recommendations for applying interpretability techniques to DRL agents.

\section{Interpretability Techniques}
Neural network (NN) has achieved great success in various fields, benefiting from its powerful feature representation capabilities. However, compared with hand-crafted feature descriptors, NN lacks interpretability, and it treats the training as black boxes. Therefore, it is important that we are able to explain and understand what the NN has learned before deploying it in real-world applications, where safety and reliability are crucial. In this work, we focus on understanding the NN in a DRL training paradigm. To do so, various interpretability methods have been considered, which include saliency maps, unit ablations, layer re-initialisation and recurrent ablations. 
\label{ch6:sec_interp}
\subsection{Saliency Maps}
\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/occ_jaco_baseline}
    \subcaption{Occ. (Gaussian)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/average_map_jaco_large.png}
    \subcaption{Avg. $\mathbf{x}^{base}$}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/occ_jaco_average_map.png}
    \subcaption{Occ. (avg. $\mathbf{x}^{base}$)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/average_map_mask.png}
    \subcaption{Avg. $\mathbf{x}^{base}$ mask}
  \end{subfigure}
  \caption{Occlusion-based saliency map method (a) using Gaussian mask (c) our improved method using an average image (b, d). Occlusion-based method using Gaussian blur masks (a) shows extra artifacts. Our improved saliency map method highlights important regions in the result by using average baseline image created from plentiful trajectories (c), (d) shows the usage of the average baseline image.}
  \label{fig:saliency_baseline}
\end{figure}

Saliency maps are widely used to understand and explain the output of NNs. Similarly to previous work on understanding DRL agents~\cite{greydanus2018visualizing}, we also use an occlusion-based approach for interpreting. The occlusion-based method~\cite{zeiler2014visualizing} generally adopts a gray mask to scan over the entire input image, and then measures the variation of the output in each masked region to identify which areas have significant influences on the output of the NN. The variation of the performance is also called saliency value, and it can be expressed as:
\begin{align}
S_{m, n} = \lVert F(\mathbf{x}) - F(\mathbf{x}_{m, n}^{occ}) \rVert_2,
\end{align}
where $(m, n)$ is the location on the image, $F(\mathbf{x})$ is the output of the original input, and $F(\mathbf{x}_{m, n}^{occ})$ is the output of the input with an occlusion mask at location $(m, n)$. In the work of~\cite{greydanus2018visualizing}, which aims to understand DRL agents in Atari games, the gray mask is replaced with a Gaussian mask. Because the agent only receives gray scale images in the training and testing. Thus, the gray mask can be recognised as a part of the gray object, and the Gaussian blur mask can introduce spatial uncertainty to find informative regions. 

However, the agent receives an RGB image as the input in our task. As a result, the occlusion-based method using Gaussian blur masks cannot highlight any important regions (see Figure~\ref{fig:saliency_baseline}a), and thus fails to provide any valuable information to understand the agent. Atrey \textit{et al.}~\cite{atrey2020exploratory} identify this issue of applying saliency methods to DRL agents as modifying observations in a manner that is incongruent with the true environment's state and generative process, and proposed intervening directly in the environment to employ counterfactual methods. However, in general, this level of control over the environment is not easy. In this work, we are more interested in task-specific objects in the environment, such as the robot arm and the red ball. To avoid interference from the occlusion-based masks on unimportant areas, we first average the frames of approximately 1,000 trajectories to get a baseline image (see Figure~\ref{fig:saliency_baseline} (b)). Then, we use this baseline image to generate the mask in the corresponding locations (see Figure~\ref{fig:saliency_baseline}d). Finally, we can get an improved saliency map, which highlights the informative regions in the input image  (see Figure~\ref{fig:saliency_baseline}c).
%Inspired by the work which use reference images for the calculation of interpreta

Another recent study introduced a more robust DRL-specific saliency method~\cite{puri2020explain}, but it is only applicable to agents which learn a state-action value function over discrete action spaces.
%\subsection{Statistical and Structural Weight Characterisations}
\subsection{Unit Ablations}
Unit ablations evaluate the importance of a single filter in the convolutional layer by masking it to zero and examining the variations in the performance. In general, an important filter can lead to a significant performance drop. While more sophisticated unit ablations have been applied in DRL~\cite{meyes2020you}, {this has only been in the context of a control task with a low-dimensional symbolic state space}.
% Layer initialisation
\subsection{Layer Re-initialisation}
Layer re-initialisation~\cite{zhang2019all} is an extension of unit ablations, which is proposed to test the re-initialisation robustness of the trained neural network. The primary idea is to replace the parameters of a single fully trained layer with the parameters in the previous training epochs, and observe the effect on the performance of the network. In this work, the parameters of each layer can be expressed as: $\{\theta_{1}, \theta_{2}, ..., \theta_{L}\}$, where $L$ is the number of layers of the network, and $\theta_{l}^{t}$ represents the parameters of the $l^\text{th}$ layer in the training epoch $t\in [1,T]$. For example, when training is finished, to evaluate the robustness of a single layer $l$ at training epoch $t$, we can re-initialise the parameters in the layer by: $\theta_{l}^{T} \leftarrow \theta_{l}^{t}$. Furthermore, we can also measure the changes of parameters in the $\ell_{\infty}$ and $\ell_{2}$-norm. Layer re-initialisation allows us to identify the critical layers in the network structure, and these layers are greatly affected by the parameters re-initialisation. Furthermore, with a similar capacity of network, we can also investigate the task complexity by using re-initialisation robustness.

\subsection{Recurrent Ablation}
Recurrent ablation is used to evaluate if the recurrent layer (e.g., long short-term memory (LSTM)) is utilised in a network structure. To do so, we set the hidden state to a constant value. If the performance of the agent is decreased to some extent, it means the recurrent layer helps to perform the task, otherwise, the recurrent layer is redundant in this structure. While it is possible to investigate the hidden states of DRL agents over time, visualising and inspecting a high-dimensional time series can be difficult, even for domain experts~\cite{jaunet2020drlviz}.

\section{Experiments}
\label{ch6:exp}
\subsection{Environments}
In the experiment, we create two simulated environments with different robot arms (see Figure~\ref{fig:robot_setups}): 1) Fetch robot arm and 2) Kinova Jaco robot arm, to accomplish a target reaching task, where the agent needs to control the robot arm's gripper to reach a specific object (e.g., the red ball) in each episode by using visual RGB inputs with or without an extra proprioceptive input (e.g., joint position, joint velocity, and joint angles). Furthermore, these two robot arms have different appearances and control schemes, which provide us with enriched experimental conditions.
These robot environments are commonly used within the DRL literature~\cite{andrychowicz2017hindsight, gu2017deep, rusu2017sim} and, therefore, adopting these in our experiments enables both comparisons to prior work and applicability to the DRL field.

\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/fetch_vol_resize.png}
    \subcaption{Fetch enviroment}
    \label{fig:fetch_volumn}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/fetch_view.png}
    \subcaption{Fetch visual input}
    \label{fig:fetch_view}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/jaco_vol_resize.png}
    \subcaption{Jaco environment}
    \label{fig:jaco_volumn}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/jaco_top.png}
    \subcaption{Jaco visual input}
    \label{fig:jaco_view}
  \end{subfigure}
  \caption{(a) Fetch environment, (b) an example of Fetch visual input, (c) Jaco environment, and (d) an example of Jaco visual input. \td{The blue cuboids indicate the sampling space of the target objects.}}
  \label{fig:robot_setups}
\end{figure}

The Fetch robot environment includes a 7 degrees-of-freedom (DOF) robot arm, and it is modified based on the model provided in OpenAI Gym~\cite{plappert2018multi}. In our Fetch environment, we revise the original proprioceptive input, where we delete the velocity and position of the gripper and the position of the target. It now only contains the position and velocity of 7 joints of the robot arm. In order to acquire visual inputs, we also add an extra RGB camera in front of the robot arm (see Figure~\ref{fig:fetch_view}). The control scheme remains the same, which is the relative position of the gripper in x, y and z axis. The target object is placed on the table. During training, the position of the target object is randomly sampled in the 2D space (see blue cuboid in Figure~\ref{fig:fetch_volumn}) of the table surface. During testing, 80 fixed uniformly distributed positions are used to evaluate the performance of the agent.

The Jaco robot environment includes a 6 DoF robot arm (the gripper is disabled). The proprioceptive input contains the position and velocity of 6 joints. An RGB camera is placed on the top of the robot arm to provide visual inputs (see Figure~\ref{fig:jaco_view}). The action space is the 6 joint velocities of the robot arm. During training, the position of the target object is randomly sampled in a 3D space (see blue cuboid in Figure~\ref{fig:jaco_volumn}). During testing, 250 uniformly distributed fixed positions are used to evaluate the performance of the agent.

The maximum step of each episode is up to 100 in training, which is used to encourage the agent to have enough exploration in the environment. The maximum step of each episode is limited to 20 in testing, which is used to avoid random actions to reach the target. The reward function is similar to Equation~\eqref{eq:sparse_reward}, the agent will achieve a +1 reward if the distance between the gripper and the target is smaller than a threshold $\epsilon$ (in this chapter, $\epsilon=$10cm), otherwise, it only gets 0. More details of these two environments are provided in Table~\ref{tbl:exp_setup}.

\begin{table}[h!]
  \centering
  \begin{tabular}{c|cc}
    \toprule
    Setting & Fetch & Jaco \\
    \midrule
    Active (Total) DoF & 7 (8) & 6 (9) \\
    Target Range & $21 \times 31 \text{cm}^2$ & $40 \times 40 \times 40 \text{cm}^3$ \\
    Num. Test Targets & 80 & 250 \\
    Vision Input & $3 \times 64 \times 64$ & $3 \times 64 \times 64$ \\
    Proprioceptive Inputs & 30 & 18 \\
    Control Type & Position & Velocity \\
    Num. Actions & 3 & 6 \\
    Action Discretisation & 5 & 5 \\
    Control Frequency & 6.67Hz & 6.67Hz \\
    \bottomrule
  \end{tabular}
  \caption{The details of the experimental setup of Fetch and Jaco environments.}
  \label{tbl:exp_setup}
\end{table}

\subsection{Training Configurations}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.85\linewidth]{figures/chapter6/axes.png}
  \caption{The 8 different experimental conditions: 2 robots (Fetch vs. Jaco), 2 input modalities (vision vs. vision + proprioception), 2 visual conditions (DR vs. no DR).}
  \label{fig:axes}
\end{figure}
We setup 8 different configurations to cover different possible training strategies and provide a comprehensive comparison. There include different combinations over three primitives: 1) using Fetch robot arm or Jaco robot arm, 2) using visual input only or visual input + proprioceptive input, and 3) training with or without domain randomisation (DR) (see Figure~\ref{fig:axes}).

{The two robot arms have different morphologies and control schemes. In addition,  we also set different target spaces (2D and 3D) to control the difficulties of the tasks. Thus, we can obtain some indication on whether the results and conclusions are generalised.}

For real-world deployment, one would want to use as many sensors as it is feasible to use. However, we are interested in how the absence of specific input modalities can affect learning, as this changes how the agent is able to extract information from the environment. For example, while one might expect robots equipped with proprioceptive sensors to use these alone for pose estimation, agents can also learn to utilise vision, if extracting a pose from images is not too difficult.

Finally, analogously to how \emph{data augmentation}~\cite{lecun1998gradient,shorten2019survey} can be used to improve the generalisation of models in supervised learning settings, \emph{domain randomisation} (Figure \ref{fig:dr_example}) is a common training paradigm in ``sim2real'' approaches for learning real-world robots control~\cite{andrychowicz2018learning,james2017transferring,peng2018sim,sadeghi2017cad2rl,tobin2017domain}. In DR, various properties of the simulation are varied, altering anything from the positions or dynamical properties of objects to their visual appearance, resulting in an expansion of the training domain. We therefore test how the agents trained with or without DR differ. Supporting previous results, we show that agents trained with DR are more robust to out-of-distribution (OoD) perturbations. In our experiments, we use a standard visual DR setup. At each timestep, we randomise all of the components in the environment by changing/adding different textures, colour gradients, RGB colours, and noise, except for the target object. More details about the DR textures are described in Table~\ref{tbl:random_summary} and visualised in Figure~\ref{fig:random_vis}.

%\subsection{Domain Randomisation}
\begin{table}[h!]
  \centering
  \resizebox{\linewidth}{!}{
  \begin{tabular}{c|c|c}
    \toprule
    No. & Option & Description \\
    \midrule
   1 & checkerboard & Render checkerboard pattern with two random RGB colours\\
   2 & gradient & Render gradient in vertical/horizontal direction with two random RGB colours\\
   3 & flat\_rgb & Render single random RGB colour\\
   4 & noise & Render artificial noise with two random RGB colours\\
    \bottomrule
  \end{tabular}}
   \caption{DR textures used during training. At every environment timestep, for each component (e.g., robot arm, skybox, table; the Jaco robot has 13 components, and the Fetch robot has 19 components), the appearance of the component is rendered using a sequential sampling process. In the first step of the sampling process, one of each of the 4 possible choices is made with the same probability, then the subsequent appearance of each component is determined by a second draw, described in the rightmost column of the table. Each RGB channel is drawn uniformly from $\{0 \ldots 255\}$. The details of each option are described in Table~\ref{tbl:texture_expression}.}
\label{tbl:random_summary}
\end{table}

\begin{table}[h!]
  \centering
  \resizebox{\linewidth}{!}{
  \begin{tabular}{c|c|c}
    \toprule
    No. & Option & Expression \\
    \midrule
   1 & checkerboard & texture\_mat[x, y] = rgb1 \textbf{if} (x + y) $\%$ 2 == 0 \textbf{else} rgb2\\
   2 & gradient & texture\_mat[x, y] = (1 - p) $\cdot$ rgb1 + p $\cdot$ rgb2, vertical: p = y / (height - 1), horizontal: p = x / (width - 1)\\
   3 & flat\_rgb & texture\_mat[x, y] = rgb1\\
   4 & noise & texture\_mat[x, y] = rgb1 \textbf{if} p $>$ 0.9 \textbf{else} rgb2, p $\sim$ U(0, 1)\\
    \bottomrule
  \end{tabular}
  }
\caption{Python pseudocode describing the DR texture generators. rgb1 and rgb2 are random RGB colours; the value of each RGB channel is drawn uniformly from $\{0 \ldots 255\}$.}
\label{tbl:texture_expression}
\end{table}

\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/rand_checker.png}
    \subcaption{checkerboard}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/rand_gradient.png}
    \subcaption{gradient}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/rand_rgb.png}
    \subcaption{flat\_rgb}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/rand_noise.png}
    \subcaption{noise}
  \end{subfigure}
  \caption{Visualisation of different DR textures used during training: (a) checkerboard, (b) gradient, (c) flat\_rgb, (d) noise.}
  \label{fig:random_vis}
\end{figure}


\subsection{Network Structure and Training Details}
\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/chapter6/network-arch-td.pdf}
  \caption{The overview of the network architecture used in the experiments.}
  \label{fig:network}
\end{figure}
The network architecture follows the same actor-critic style used in the work of Rusu \textit{et al.}~\cite{rusu2017sim} (see Figure~\ref{fig:network}). The visual input is first passed through 2 convolutional layers, and then the features of the last convolutional layer are flattened into 1152-dimensional vector. Subsequently, if proprioceptive input is provided, it will be concatenated with the previous flatten vector and then sent into a fully connected layer. This is followed by a long short-term memory (LSTM) layer. Finally, the output of the LSTM layer will be sent to two independent fully connected layers to predict the policy $\pi(a_{t}|s_{t};\theta)$ and state value $V(s_{t};\theta)$, respectively. 

In the experiment, proximal policy optimisation (PPO) is used as the base RL algorithm, and 4 MPI workers are used to parallelise the training. During training, each MPI worker uses 8 threads to collect experience for 128 timesteps per epoch. PPO update per epoch is 4 with a batch size equals to 256. Adam is selected as the optimiser, and learning rate is 0.00025. Clip ratio $\epsilon$ is 0.1. Discount factor $\gamma$ is 0.99. A complete training for one model lasts 5000 epochs, and it needs to run on a GTX-1080Ti GPU for one day. For each training configuration, we train the models with 5 different seeds. The overall setup is detailed in Algorithm~\ref{alg:overall_algo}. The code and pretrained models used in this paper are available at: \url{https://github.com/TianhongDai/domain-rand-interp}.

\begin{algorithm}[h!]
\caption{PPO + DR training.}
\label{alg:overall_algo}
\begin{algorithmic}
\REQUIRE environment $E$, actor network $\pi(a|s;\theta)$, critic network $V_{\pi}(s;\theta)$, number of training epochs $N$, number of timesteps to collect data $T$, number of PPO updates $M$, on-policy transition memory $\mathcal{B}$
\FOR{epoch $= 1, 2, \ldots, N$}
  \STATE $\mathcal{B} \gets \varnothing$
  \FOR{$t = 1, 2, \ldots, T$}
    \IF{$E$ has terminated}
      \STATE Reset $E$ and receive DR \indent\indent state $s_0$
    \ENDIF
    \STATE Sample an action $a_t \sim \pi(a|s_t;\theta)$
    \STATE Execute $a_{t}$ in $E$ and receive reward $r_t$, DR next \indent\indent state $s_{t+1}$
    \STATE Store the transition $\left(s_t, a_t, r_t, s_{t+1} \right)$ in $\mathcal{B}$
  \ENDFOR
  \FOR{PPO update $= 1, 2, \cdots, M$}
    \STATE Update $\pi(a|s;\theta)$ using gradient ascent on Eq.\eqref{eq:standard_ppo}  with data from $\mathcal{B}$
    \STATE Update $V_{\pi}(s;\theta)$ using gradient descent on Eq.\eqref{eq:mse} \indent\indent with data from $\mathcal{B}$
  \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Results and Analysis}
\label{ch6:results}
\begin{figure}[h!]
  \centering
  %\hspace{0.025\textwidth}
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/training_curves/jaco_prop.pdf}
    \caption{Trained with Proprioceptive inputs}
  \end{subfigure}\hfill
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/training_curves/jaco_noprop.pdf}
    \caption{Trained without proprioceptive inputs}
  \end{subfigure}
  %\hspace{0.025\textwidth}
  \caption{The learning curves of Jaco agents trained with DR and with (a) or without (b) proprioceptive inputs. (a) the agent have similar performance in both standard and DR environments. (b) the agent has lower performance in standard environment. The solid line is the median value and the shadow area is the 95$\%$ confidence interval.}
  \label{fig:domain_shift}
\end{figure}

Before formally testing and analysing the models, they are \td{fully trained} with 8 training configurations and the same hyperparameters to achieve promising performance. However, in the Jaco environment, we also find that the model trained with visual + proprioceptive inputs and DR has a performance degradation when using standard visual inputs (no DR; see Figure~\ref{fig:domain_shift}). This is unexpected when the trained policy is transferred from complex visual scenarios to a simple visual scenario, and this observation demonstrates that this agent is overfitting to DR visual inputs. Since each model has different performance in the standard environment, it is more meaningful to compare the performance variation of individual models under different test conditions.

\subsection{Test Conditions}
\begin{figure*}[h!]
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/test_observations/standard}
    \caption{Standard}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/test_observations/colour_b}
    \caption{Colour}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/test_observations/shape}
    \caption{Shape}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/test_observations/illumination}
    \caption{Illumination (Lvl.)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/test_observations/illumination_dir}
    \caption{Illumination (Dir.)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/test_observations/noise}
    \caption{Noise}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/test_observations/reflection}
    \caption{Reflection}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/test_observations/translation}
    \caption{Translation}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/test_observations/invisible}
    \caption{Invisibility}
  \end{subfigure}
  \caption{The visualisation of different test conditions in the Fetch environment.}
  \label{fig:tests}
\end{figure*}
We develop 8 different test conditions to evaluate the generalisation ability of the trained agents. Figure~\ref{fig:tests} shows an overview of 8 test conditions in the Fetch environment. The details of each test condition are listed below:
\begin{enumerate}
\item[a)] \textbf{Standard}: This is the default setting of the environment, which is used to evaluate the baseline performance of the agent for further comparison. 

\item[b)] \textbf{Colour}: This condition introduces a distractor object, which has the same shape and size as the target object but has a different colour. It aims to evaluate the robustness of the agent when there exists an extra object with different colours. We use five different colours: yellow, blue, green, brown and purple. Some of these colours have a red component (yellow, brown and purple), while others do not (blue and green).

\item[c)] \textbf{Shape}: This condition introduces a red distractor object, which has a different shape to the target object. It aims to evaluate the robustness of the agent when there exists an extra object with different shapes. We use four different shapes: cube, ellipsoid, rectangle and diamond.

\item[d)] \textbf{Illumination (Lvl.)}: This condition changes the diffuse intensity of the source of light. We use five illumination levels: 0.4 to 0.0 for the Fetch environment, and 0.9 to 0.1 for the Jaco environment.

\item[e)] \textbf{Illumination (Dir.)}: This condition changes the location of the source of light. We use five different locations for both Fetch and Jaco environments.

\item[f)] \textbf{Noise}: This condition adds Gaussian noise $\mathcal{N}(0, 0.25)$ to the visual inputs. This noise is not the same as the one generated in DR.

\item[g)] \textbf{Reflection}: This condition makes the table of the Fetch environment and the floor of the Jaco environment reflective, and it can produces mirror-like reflections in these surfaces.

\item[h)] \textbf{Translation}: This condition shifts the RGB camera's location. We use 5 locations: -20 to 20cm in the y direction for the Fetch environment, and -20 to 20cm in the x direction for the Jaco environment.

\item[i)] \textbf{Invisibility}: This condition is an extreme case, where the robot arm becomes transparent, and it is only used to investigate the importance of the visual inputs to localise the robot arm.
\end{enumerate}

\subsection{Quantitative Results on Testing Conditions}
We first discuss how local visual changes affect the performance of these trained agents. Table~\ref{tbl:tests} demonstrates the performance of agents with different training configurations under different test conditions. For both the Fetch and Jaco environments, the agents trained with DR perform slightly worse than those trained without DR in the standard test condition. However, agents trained with DR show strong robustness to both kinds of distractors. In Table~\ref{tbl:diff_colours}, we also provide the performance of the agents to different colours and different shapes of the distractors.

In the Fetch environment, the trained agents are more robust to the colour distractors. However, the shape distractors decrease the performance of the agents trained without DR, especially for non-DR agents trained with proprioceptive inputs, which drops to a 0.48 success rate. This implies that the non-DR agent trained with proprioceptive inputs localises the target object primarily by colour information and partly with shape information. The non-DR agent trained with visual inputs only merely drops 0.17 success rate. \td{This is because the agent trained with only visual inputs requires better representation/feature learning to localise the position of the robot arm (i.e., self-localisation), and these learned features can help the agent to distinguish between different shapes and colours.}

In the Jaco environment, both non-DR agents show performance degradation when there exists a distractor with red components, and both DR agents have significant performance drops when the shape of the distractor is a diamond, which has a similar appearance to a sphere in a low-resolution viewed input. It also indicates that the non-DR agents prefer to localise the target object via red colour.

In addition, we also test the effect of the distractors on the agent in different positions. In Table~\ref{tbl:vary_distractor}, it only has a few influences on performance.

\begin{table}[H]
%\begin{sidewaystable}[h!]
  %\label{tbl:tests}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{c|cc|c|cc|cc|ccc|c}
    \toprule
    Robot & DR & Prop. & Standard & Colours & Shapes & Illumination (Lvl.) & Illumination (Dir.) & Noise & Reflection & Translation & Invisibility\\
    \midrule
    Fetch & {\xmark}   & {\xmark}   & 1.000$\pm 0.000$ & 0.993$\pm 0.004$ & 0.827$\pm 0.056$ & 0.694$\pm 0.049$ & 0.539$\pm 0.069$ & 0.980$\pm 0.006$ & 0.447$\pm 0.039$ & 0.270$\pm 0.079$ & 0.000$\pm 0.000$\\
    Fetch & {\xmark}   & {\cmark} & 1.000$\pm 0.000$ & 0.937$\pm 0.027$ & 0.524$\pm 0.087$ & 0.593$\pm 0.064$ & 0.491$\pm 0.066$ & 0.988$\pm 0.004$ & 0.570$\pm 0.078$ & 0.231$\pm 0.078$ & 0.000$\pm 0.000$\\
    Fetch & {\cmark} & {\xmark}   & 0.983$\pm 0.004$ & 0.974$\pm 0.004$ & 0.939$\pm 0.018$ & 0.957$\pm 0.008$ & 0.740$\pm 0.058$ & 0.985$\pm 0.007$ & 0.972$\pm 0.011$ & 0.480$\pm 0.064$ & 0.000$\pm 0.000$\\
    Fetch & {\cmark} & {\cmark} & 0.997$\pm 0.002$ & 0.997$\pm 0.001$ & 0.978$\pm 0.009$ & 0.992$\pm 0.002$ & 0.883$\pm 0.038$ & 0.970$\pm 0.008$ & 0.985$\pm 0.005$ & 0.536$\pm 0.063$ & 0.023$\pm 0.015$\\
    \hline
    Jaco  & {\xmark}   & {\xmark}   & 0.995$\pm 0.003$ & 0.772$\pm 0.058$ & 0.367$\pm 0.069$ & 0.966$\pm 0.012$ & 0.838$\pm 0.035$ & 0.635$\pm 0.028$ & 0.734$\pm 0.032$ & 0.556$\pm 0.063$ & 0.000$\pm 0.000$\\
    Jaco  & {\xmark} & {\cmark} & 0.995$\pm 0.001$ & 0.808$\pm 0.044$ & 0.382$\pm 0.064$ & 0.876$\pm 0.032$ & 0.911$\pm 0.014$ & 0.478$\pm 0.059$ & 0.618$\pm 0.061$ & 0.688$\pm 0.048$ & 0.001$\pm 0.001$\\
    Jaco  & {\cmark} & {\xmark}   & 0.650$\pm 0.056$ & 0.652$\pm 0.022$ & 0.593$\pm 0.031$ & 0.643$\pm 0.027$ & 0.549$\pm 0.032$ & 0.575$\pm 0.040$ & 0.429$\pm 0.060$ & 0.310$\pm 0.041$ & 0.007$\pm 0.002$\\
    Jaco  & {\cmark} & {\cmark} & 0.991$\pm 0.004$ & 0.989$\pm 0.002$ & 0.843$\pm 0.061$ & 0.816$\pm 0.043$ & 0.882$\pm 0.031$ & 0.896$\pm 0.007$ & 0.946$\pm 0.006$ & 0.658$\pm 0.054$ & 0.916$\pm 0.022$\\
    \bottomrule
  \end{tabular}}
  \caption{Test performance of all models with local visual changes (distractors), global visual changes, and invisibility (visual self-localisation test). Colour and shape results are averaged over 9 different distractor locations, as well as all colours and shapes, respectively. Checkmarks and crosses indicate enabling/disabling DR and proprioceptive inputs (Prop.), respectively. Mean $\pm$ standard error are calculated over all models (seeds) and test target locations.}
  \label{tbl:tests}
%\end{sidewaystable}
\end{table}

\begin{table}[h!]
%\begin{sidewaystable}[h!]
  \resizebox{\linewidth}{!}{
  \begin{tabular}{c|cc|ccccc|cccc}
    \toprule
    Robot & DR & Prop. & Yellow & Blue & Green & Brown & Purple & Cube & Ellipsoid & Rectangle & Diamond\\
    \midrule
    Fetch & {\xmark}   & {\xmark}   & 0.993$\pm 0.007$ & 1.000$\pm 0.000$ & 1.000$\pm 0.000$ & 0.978$\pm 0.017$ & 0.995$\pm 0.004$ & 0.775$\pm 0.085$ & 0.993$\pm 0.007$ & 0.960$\pm 0.036$ & 0.580$\pm 0.142$\\
    Fetch & {\xmark}   & {\cmark} & 0.875$\pm 0.088$ & 1.000$\pm 0.000$ & 1.000$\pm 0.000$ & 0.825$\pm 0.069$ & 0.983$\pm 0.006$ & 0.243$\pm 0.064$ & 0.980$\pm 0.006$ & 0.783$\pm 0.079$ & 0.090$\pm 0.048$\\
    Fetch & {\cmark} & {\xmark}   & 0.970$\pm 0.011$ & 0.980$\pm 0.006$ & 0.975$\pm 0.006$ & 0.973$\pm 0.010$ & 0.973$\pm 0.011$ & 0.913$\pm 0.042$ & 0.975$\pm 0.009$ & 0.965$\pm 0.013$ & 0.905$\pm 0.048$\\
    Fetch &{\cmark} & {\cmark} & 0.995$\pm 0.003$ & 0.998$\pm 0.002$ & 1.000$\pm 0.000$ & 0.998$\pm 0.002$ & 0.995$\pm 0.003$ & 0.963$\pm 0.020$ & 0.998$\pm 0.002$ & 0.995$\pm 0.003$ & 0.958$\pm 0.022$\\
    \hline
    Jaco  & {\xmark}   & {\xmark}   & 0.281$\pm 0.067$ & 0.871$\pm 0.036$ & 0.994$\pm 0.003$ & 0.730$\pm 0.095$ & 0.984$\pm 0.008$ & 0.274$\pm 0.077$ & 0.407$\pm 0.098$ & 0.759$\pm 0.066$ & 0.027$\pm 0.006$\\
    Jaco  & {\xmark}   & {\cmark} & 0.451$\pm 0.040$ & 0.941$\pm 0.032$ & 0.993$\pm 0.002$ & 0.706$\pm 0.064$ & 0.950$\pm 0.028$ & 0.258$\pm 0.044$ & 0.440$\pm 0.058$ & 0.784$\pm 0.044$ & 0.045$\pm 0.009$\\
    Jaco  & {\cmark} & {\xmark}   & 0.640$\pm 0.046$ & 0.655$\pm 0.055$ & 0.651$\pm 0.055$ & 0.668$\pm 0.046$ & 0.648$\pm 0.046$ & 0.636$\pm 0.040$ & 0.642$\pm 0.035$ & 0.666$\pm 0.047$ & 0.426$\pm 0.054$\\
    Jaco  & {\cmark} & {\cmark} & 0.987$\pm 0.005$ & 0.990$\pm 0.003$ & 0.987$\pm 0.003$ & 0.990$\pm 0.003$ & 0.990$\pm 0.003$ & 0.970$\pm 0.017$ & 0.970$\pm 0.011$ & 0.990$\pm 0.003$ & 0.441$\pm 0.128$\\
    \bottomrule
  \end{tabular}}
  \caption{Test performance of all models with local visual changes (distractors). Checkmarks and crosses indicate enabling/disabling DR and proprioceptive inputs (Prop.), respectively. Mean $\pm$ standard error are calculated over all models (seeds), test target locations, and 9 distractor locations.}
  \label{tbl:diff_colours}
%\end{sidewaystable}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{c|cc|cc}
    \toprule
    Robot & DR & Prop. & Colours & Shapes\\
    \midrule
    Fetch & {\xmark}   & {\xmark}   & 0.993$\pm 0.004$ & 0.567$\pm 0.047$\\
    Fetch & {\xmark}   & {\cmark} & 0.937$\pm 0.027$ & 0.507$\pm 0.047$\\
    Fetch & {\cmark} & {\xmark}   & 0.974$\pm 0.004$ & 0.639$\pm 0.051$\\
    Fetch & {\cmark} & {\cmark} & 0.997$\pm 0.001$ & 0.622$\pm 0.050$\\
    \hline
    Jaco  & {\xmark} & {\xmark}   & 0.772$\pm 0.058$ & 0.567$\pm 0.047$\\
    Jaco  & {\xmark} & {\cmark} & 0.808$\pm 0.044$ & 0.507$\pm 0.047$\\
    Jaco  & {\cmark} & {\xmark}   & 0.652$\pm 0.022$ & 0.639$\pm 0.051$\\
    Jaco  & {\cmark} & {\cmark} & 0.989$\pm 0.002$ & 0.622$\pm 0.050$\\
    \bottomrule
  \end{tabular}
  \caption{Test performance of a single model with distractors locations varying over 9 different on the ground plane (Jaco) and table (Fetch). Checkmarks and crosses indicate enabling/disabling DR and proprioceptive inputs (Prop.), respectively. Mean $\pm$ standard error are calculated for the best model (seed), over all test target locations and all distractor locations.}
  \label{tbl:vary_distractor}
\end{table}


Next, we discuss how global visual changes affect the performance of these trained agents. From Table~\ref{tbl:tests}, in this case, the DR agents show better robustness but still sacrifice a lot of performance in many test conditions.

{In the Illumination (Lvl.) condition, the performance of all trained agents decreases as the light intensity reduces ({see Table~\ref{tbl:diff_levels}}). In the Fetch environment, the DR agents are more robust to variations of the illumination intensity. In the Jaco environment, however, the DR agent trained with visual inputs only is more robust in this case. This is probably because the agent trained with visual inputs must learn better features for self-localisation, thus the learned features are more robust to the change of the environment.}

In the Illumination (Dir.) condition, the performance of all agents decreases as the direction of the light source changes (see Table~\ref{tbl:diff_dirs}). Similarly, in the Fetch environment, the DR agents are more robust. In the Jaco environment, compared to training with DR, proprioceptive inputs would be more helpful for robustness.

\begin{table}[H]
  \resizebox{0.9\linewidth}{!}{
  \begin{tabular}{c|cc|ccccc}
    \toprule
    Robot & DR & Prop. & 0.0/0.1 & 0.1/0.3 & 0.2/0.5 & 0.3/0.7 & 0.4/0.9 \\
    \midrule
    Fetch & {\xmark}   & {\xmark}   & 0.468$\pm 0.670$ & 0.513$\pm 0.081$ & 0.643$\pm 0.076$ & 0.860$\pm 0.047$ & 0.988$\pm 0.009$ \\
    Fetch & {\xmark}   & {\cmark} & 0.325$\pm 0.115$ & 0.383$\pm 0.112$ & 0.508$\pm 0.110$ & 0.775$\pm 0.069$ & 0.973$\pm 0.013$ \\
    Fetch & {\cmark} & {\xmark}   & 0.893$\pm 0.013$ & 0.948$\pm 0.018$ & 0.975$\pm 0.004$ & 0.985$\pm 0.004$ & 0.983$\pm 0.004$ \\
    Fetch & {\cmark} & {\cmark} & 0.983$\pm 0.006$ & 0.990$\pm 0.002$ & 0.995$\pm 0.003$ & 0.995$\pm 0.003$ & 0.995$\pm 0.003$ \\
    \hline
    Jaco  & {\xmark} & {\xmark}   & 0.874$\pm 0.034$ & 0.972$\pm 0.008$ & 0.990$\pm 0.001$ & 0.996$\pm 0.001$ & 0.996$\pm 0.001$ \\
    Jaco  & {\xmark} & {\cmark} & 0.587$\pm 0.043$ & 0.846$\pm 0.022$ & 0.966$\pm 0.007$ & 0.989$\pm 0.002$ & 0.994$\pm 0.001$ \\
    Jaco  & {\cmark} & {\xmark}   & 0.473$\pm 0.049$ & 0.626$\pm 0.042$ & 0.707$\pm 0.039$ & 0.711$\pm 0.046$ & 0.698$\pm 0.041$ \\
    Jaco  & {\cmark} & {\cmark} & 0.442$\pm 0.018$ & 0.713$\pm 0.013$ & 0.946$\pm 0.012$ & 0.986$\pm 0.002$ & 0.994$\pm 0.002$ \\
    \bottomrule
  \end{tabular}}
  \caption{Test performance of all models with different global illumination intensities; illumination is specificied as Fetch/Jaco. Checkmarks and crosses indicate enabling/disabling DR and proprioceptive inputs (Prop.), respectively. Mean $\pm$ standard error are calculated over all models (seeds) and test target locations.}
  \centering
  \label{tbl:diff_levels}
\end{table}

\begin{table}[H]
  \resizebox{0.9\linewidth}{!}{
  \begin{tabular}{c|cc|ccccc}
    \toprule
    Robot & DR & Prop. & 0.0,0.0,-1.0/ & -1.0,0.0,0.0/ & -1.0,0.0,-1.0/ & 0.0,-1.0,0.0/ & 0.0,1.0,1.0/   \\
          &    &       & 0.0,0.0,-1.3  & -1.3,0.0,0.0  & -1.3,0.0,-1.3  & 0.0,-1.3,-1.3 & -1.3,-1.3,-1.3 \\
    \midrule
    Fetch & {\xmark}   & {\xmark}   & 1.000$\pm 0.000$ & 0.020$\pm 0.004$ & 0.593$\pm 0.095$ & 0.518$\pm 0.091$ & 0.563$\pm 0.066$ \\
    Fetch & {\xmark}   & {\cmark} & 1.000$\pm 0.000$ & 0.068$\pm 0.041$ & 0.493$\pm 0.074$ & 0.453$\pm 0.079$ & 0.440$\pm 0.066$ \\
    Fetch & {\cmark} & {\xmark}   & 0.983$\pm 0.004$ & 0.338$\pm 0.117$ & 0.850$\pm 0.087$ & 0.763$\pm 0.097$ & 0.765$\pm 0.075$ \\
    Fetch & {\cmark} & {\cmark} & 0.998$\pm 0.002$ & 0.540$\pm 0.076$ & 0.948$\pm 0.025$ & 0.985$\pm 0.009$ & 0.945$\pm 0.021$ \\
    \hline
    Jaco  & {\xmark}   & {\xmark}   & 0.995$\pm 0.003$ & 0.818$\pm 0.022$ & 0.650$\pm 0.121$ & 0.805$\pm 0.044$ & 0.922$\pm 0.015$ \\
    Jaco  & {\xmark}   & {\cmark} & 0.995$\pm 0.001$ & 0.852$\pm 0.019$ & 0.930$\pm 0.029$ & 0.855$\pm 0.023$ & 0.922$\pm 0.011$ \\
    Jaco  & {\cmark} & {\xmark}   & 0.650$\pm 0.056$ & 0.608$\pm 0.044$ & 0.455$\pm 0.070$ & 0.506$\pm 0.068$ & 0.529$\pm 0.074$ \\
    Jaco  & {\cmark} & {\cmark} & 0.991$\pm 0.004$ & 0.586$\pm 0.017$ & 0.932$\pm 0.016$ & 0.968$\pm 0.010$ & 0.934$\pm 0.016$ \\
    \bottomrule
  \end{tabular}}
  \caption{Test performance of all models with different main illumination directions; direction is specified as Fetch/Jaco. Checkmarks and crosses indicate enabling/disabling DR and proprioceptive inputs (Prop.), respectively. Mean $\pm$ standard error are calculated over all models (seeds) and test target locations.}
  \centering
  \label{tbl:diff_dirs}
\end{table}

For the case of additive Gaussian noise, the agents trained in the Fetch environment are all robust to the noise. However, in the Jaco environment, the non-DR agents have approximately 30$\%$ performance drop, while the DR agents only have 10$\%$.

In the reflection condition, the agents trained with DR are more robust. However, in the Fetch environment, it decreases the performance of the non-DR agents by 50$\%$. While in the Jaco environment, there is not a significant influence on the performance of the non-DR agents. One possible explanation is the difference in the sizes of the robot arm. The Jaco robot arm is relatively small in the simulation, so its reflection has little change on the visual inputs.

In the translation condition, the performance of all agents reduces heavily (see Table~\ref{tbl:diff_trans}). In the Fetch environment, just as before, DR agents exhibit better robustness. In the Jaco environment, non-DR and DR agents have similar performance, which reflects that these agents are able to learn the extent of translation invariance for the policies. This could be a benefit for the target reaching task in a 3D space, which leads to a more generalisable spatial representation.

\begin{table}[H]
  \resizebox{\linewidth}{!}{
  \begin{tabular}{c|cc|ccccc}
    \toprule
    Robot & DR & Prop. & -20cm & -10cm & 0cm & 10cm & 20cm \\
    \midrule
    Fetch & {\xmark}   & {\xmark}   & 0.013$\pm 0.004$ & 0.168$\pm 0.101$ & 1.000$\pm 0.000$ & 0.163$\pm 0.091$ & 0.008$\pm 0.004$ \\
    Fetch & {\xmark}   & {\cmark} & 0.043$\pm 0.027$ & 0.075$\pm 0.039$ & 1.000$\pm 0.000$ & 0.035$\pm 0.010$ & 0.000$\pm 0.000$ \\
    Fetch & {\cmark} & {\xmark}   & 0.268$\pm 0.042$ & 0.625$\pm 0.056$ & 0.983$\pm 0.004$ & 0.433$\pm 0.056$ & 0.093$\pm 0.040$ \\
    Fetch & {\cmark} & {\cmark} & 0.358$\pm 0.051$ & 0.685$\pm 0.033$ & 0.998$\pm 0.002$ & 0.488$\pm 0.094$ & 0.153$\pm 0.055$ \\
    \hline
    Jaco  & {\xmark}   & {\xmark}   & 0.107$\pm 0.030$ & 0.570$\pm 0.065$ & 0.995$\pm 0.003$ & 0.714$\pm 0.022$ & 0.394$\pm 0.055$ \\
    Jaco  & {\xmark}   & {\cmark} & 0.464$\pm 0.062$ & 0.826$\pm 0.026$ & 0.995$\pm 0.001$ & 0.757$\pm 0.022$ & 0.399$\pm 0.040$ \\
    Jaco  & {\cmark} & {\xmark}   & 0.175$\pm 0.025$ & 0.265$\pm 0.049$ & 0.650$\pm 0.056$ & 0.321$\pm 0.030$ & 0.141$\pm 0.034$ \\
    Jaco  & {\cmark} & {\cmark} & 0.338$\pm 0.034$ & 0.853$\pm 0.014$ & 0.991$\pm 0.004$ & 0.751$\pm 0.021$ & 0.356$\pm 0.029$ \\
    \bottomrule
  \end{tabular}}
  \caption{Test performance of all models with different levels of camera translation. Checkmarks and crosses indicate enabling/disabling DR and proprioceptive inputs (Prop.), respectively. Mean $\pm$ standard error are calculated over all models (seeds) and test target locations.}
  \label{tbl:diff_trans}
\end{table}


Finally, in the case of the arm being invisible condition, the performance of almost all agents is reduced to zero, except for the Jaco DR agent trained with proprioceptive inputs. For those failure cases, the agents need to infer the position of the robot arm visually. For the Jaco DR agent trained with proprioceptive inputs, the position of the robot arm is estimated by using only proprioceptive inputs.

From these results, we can conclude that, with the existing visual DR setting that randomises textures and colours, the agents can be made more robust to local visual changes, but less robust to global visual changes, such as the change of illumination intensity or the translation of the RGB camera. It is reasonable, because we do not include these variations in the DR configuration. Through finding such failure scenarios for the agent, it can also help us improve the configuration of DR.

% Saliency Map
\subsection{Saliency Maps}
We first employ the saliency map technique to infer which part of the visual input will affect the output of the policy network. Then, our proposed occlusion-based saliency map is used to investigate the trained agents in standard condition, colour change condition (yellow), and shape change condition (cube), respectively. 

Figure~\ref{fig:saliency_fetch_distractor} demonstrates the saliency maps of the trained agents in the Fetch environment. From the results, DR agents in Figure~\ref{fig:saliency_fetch_distractor}j-l can self-localise via proprioceptive inputs. The others need to use visual inputs for self-localisation, where the saliency maps display highlighted regions in the robot arm. In Figure~\ref{fig:saliency_fetch_distractor}d-f, although the proprioceptive inputs have been provided to non-DR agents, these agents still use the visual inputs for self-localisation, which means not all inputs will be utilised efficiently. In addition, the saliency maps of the non-DR agents highlight more regions on the distractors (see Figure~\ref{fig:saliency_fetch_distractor}a-f), while the saliency maps of the DR agents do not (see Figure~\ref{fig:saliency_fetch_distractor}g-k), except for the case in Figure~\ref{fig:saliency_fetch_distractor}l. 

Figure~\ref{fig:saliency_jaco_distractor} shows the saliency maps of the trained agents in the Jaco environment, which are more in line with our expectations. When the proprioceptive inputs are not available, the saliency maps have more highlighted regions on the robot arm for self-localisation. Conversely, the saliency maps only focus on the target object. Furthermore, from Figure~\ref{fig:saliency_jaco_distractor}b,c,e,f, the non-DR agents also pay more attention to the distractors. A close observation reveals that there exist weak highlighted regions around distractors in Figure~\ref{fig:saliency_jaco_distractor}e,f and robot arm in  Figure~\ref{fig:saliency_jaco_distractor}g-i, respectively.

While these saliency maps can provide some intuitive explanations, they do not completely replace the quantitative results in Table~\ref{tbl:diff_colours}. For example, the Fetch DR agent trained with proprioception inputs pays attention to the distractors. However, it has less performance drop than the Fetch DR agent trained with visual inputs only, whose saliency map only has highlighted regions on the target ball. Therefore, we cannot rely solely on saliency maps to interpret and analyse trained agents.

\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_off/standard_visual_std}
    \subcaption{Standard (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_off/color_visual_std}
    \subcaption{Colour (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_off/shape_visual_std}
    \subcaption{Shape (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_on/standard_sensor_std}
    \subcaption{Standard (no DR, proprioception)}
  \end{subfigure}
  
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_on/color_sensor_std}
    \subcaption{Colour (no DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_on/shape_sensor_std}
    \subcaption{Shape (no DR, proprioception)}
  \end{subfigure}
  
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_off/standard_visual_random}
    \subcaption{Standard (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_off/color_visual_random}
    \subcaption{Colour (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_off/shape_visual_random}
    \subcaption{Shape (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_on/standard_sensor_random}
    \subcaption{Standard (DR, proprioception)}
  \end{subfigure}
  
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_on/color_sensor_random}
    \subcaption{Colour (DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_fetch_pro_on/shape_sensor_random}
    \subcaption{Shape (DR, proprioception)}
  \end{subfigure}
  \caption{Occlusion-based saliency maps of Fetch agents trained with (g-l) or without (a-f) DR and with (d-f, j-l) or without proprioceptive inputs (a-c, g-i) in three different test conditions (standard, colour and shape). The best Fetch model is used for each training configuration.}
  \label{fig:saliency_fetch_distractor}
\end{figure}

\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_off/standard_visual_std}
    \subcaption{Standard (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_off/color_visual_std}
    \subcaption{Colour (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_off/shape_visual_std}
    \subcaption{Shape (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_on/standard_sensor_std}
    \subcaption{Standard (no DR, proprioception)}
  \end{subfigure}
  
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_on/color_sensor_std}
    \subcaption{Colour (no DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_on/shape_sensor_std}
    \subcaption{Shape (no DR, proprioception)}
  \end{subfigure}
  
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_off/standard_visual_random}
    \subcaption{Standard (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_off/color_visual_random}
    \subcaption{Colour (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_off/shape_visual_random}
    \subcaption{Shape (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_on/standard_sensor_random}
    \subcaption{Standard (DR, proprioception)}
  \end{subfigure}
  
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_on/color_sensor_random}
    \subcaption{Colour (DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.24\columnwidth}
    \includegraphics[width=\linewidth]{figures/chapter6/distractor_saliency_jaco_pro_on/shape_sensor_random}
    \subcaption{Shape (DR, proprioception)}
  \end{subfigure}
  \caption{Occlusion-based saliency maps of Jaco models trained with (g-l) or without (a-f) DR and with (d-f, j-l) or without proprioceptive inputs (a-c, g-i) in three different test conditions (standard, colour and shape). The best Jaco model is used for each training configuration.}
  \label{fig:saliency_jaco_distractor}
\end{figure}

%\subsection{Statistical and Structural Weight Characterisations}
\subsection{Unit Ablations}

\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\textwidth]{figures/chapter6/unitablation/conv1_ablations}
    \caption{Standard test condition (Layer 1)}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\textwidth]{figures/chapter6/unitablation/conv1_ablations_noisy}
    \caption{Noise test condition (Layer 1)}
  \end{subfigure}

  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\textwidth]{figures/chapter6/unitablation/conv2_ablations}
    \caption{Standard test condition (Layer 2)}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=\textwidth]{figures/chapter6/unitablation/conv2_ablations_noisy}
    \caption{Noise test condition (Layer 2)}
  \end{subfigure}
  \caption{The performance of unit ablation tests of the trained Fetch and Jaco agents under 8 different training configurations in standard and visual noise conditions. Each point represents one unit in layer 1 (a, b) or layer 2 (c, d). The vertical bars indicate baseline performance in the test environment. The training settings correspond to the Fetch (F) and Jaco (J) robots, whether additional proprioceptive inputs are available (Prop), and if DR is activated. The best model is used for each training configuration. To be noticed that the Jaco agent trained with DR but without proprioceptive inputs has a lower baseline performance on the standard visuals than the other models (Table \ref{tbl:tests}).}
  \label{fig:unit_ablations}
\end{figure}
We use unit ablations to provide white box and quantitative analysis. We first select the best agent from each of 8 training configurations across five different seeds as our test models. Subsequently, for each test model, we mask one of the output channels to zero in the first or second convolutional layers. Then, the modified model is evaluated on both standard and noise conditions (80 fixed positions for Fetch agents and 250 fixed positions for Jaco agents). Finally, we repeat the above process until all output channels have been tested. The results of the unit ablation tests are illustrated in Figure~\ref{fig:unit_ablations}

From Figure~\ref{fig:unit_ablations}, we observe that the impact of unit ablation on the Fetch agents is small. However, the effect on the Jaco agents is relatively large, as the performance of the Jaco agents varies significantly. This is probably because of the increased complexity of the Jaco task (e.g., control scheme) and which requires more powerful channels to extract relevant information to train the agents.

% We further find that the unit ablations can improve the performance in the noise condition, even for the DR agents. \td{This may}

Furthermore, in the first convolutional layer (see Figure~\ref{fig:unit_ablations}a,b), there exist several extremely critical channels/units that can substantially affect the performance of the agent. However, there are no such channels in the second convolutional layer (see Figure~\ref{fig:unit_ablations}c,d).

\subsection{Layer Re-initialisation}
Next, we move to the layer re-initialisation test, where we evaluate models' robustness to the layer re-initialisation and the change of the parameters in $\ell_{\infty}$ and $\ell_{2}$ norm. The related plots of Fetch and Jaco agents are in Figure~\ref{fig:fetch_robustness} and Figure~\ref{fig:jaco_robustness}, respectively. Generally, after a period of training, the LSTM layer and policy layer are more robust to be re-initialised with the initial weights. For the DR agents, they show less robustness to re-initialisation from the early to middle stages of training, which indicates DR agents require longer periods to learn useful representations from tasks.

Another interesting finding is that when the proprioceptive inputs are available, the first fully connected layer (fc1) of the agents needs more extended periods for training. This is probably because the fc1 layer has to learn a sophisticated combination between the visual inputs and proprioceptive inputs. This is especially the case for the Jaco DR agent, which can use only proprioceptive inputs for self-localisation: it requires an even longer time to train the fc1 layer than other agents (see Figure~\ref{fig:jaco_robustness}j).

Finally, in almost all cases, the LSTM layer is robust to be re-initialised with the initial weights. Therefore, we suppose that the LSTM layer is redundant in the network structure for these tasks. Further experiments to verify this assumption are provided in the following subsection.

\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/visual_std/error}
    \caption{Robustness (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/visual_std/inf_dist}
    \caption{$\ell_\infty$-norm (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/visual_std/l2_dist}
    \caption{$\ell_2$-norm (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/sensor_std/error}
    \caption{Robustness (no DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/sensor_std/inf_dist}
    \caption{$\ell_\infty$-norm (no DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/sensor_std/l2_dist}
    \caption{$\ell_2$-norm (no DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/visual_random/error}
    \caption{Robustness (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/visual_random/inf_dist}
    \caption{$\ell_\infty$-norm (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/visual_random/l2_dist}
    \caption{$\ell_2$-norm (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/sensor_random/error}
    \caption{Robustness (DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/sensor_random/inf_dist}
    \caption{$\ell_\infty$-norm (DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/fetch/sensor_random/l2_dist}
    \caption{$\ell_2$-norm (DR, proprioception)}
  \end{subfigure}
  \caption{Performance of re-initialisation robustness (0 for 100$\%$ success rate, and 1 for 0$\%$ success rate), and change of parameters in $\ell_\infty$- and $\ell_2$-norm of Fetch agents. The best Fetch model is selected for each training configuration.}
  \label{fig:fetch_robustness}
\end{figure}
%\vspace{-15mm}
\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/visual_std/error}
    \caption{Robustness (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/visual_std/inf_dist}
    \caption{$\ell_\infty$-norm (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/visual_std/l2_dist}
    \caption{$\ell_2$-norm (no DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/sensor_std/error}
    \caption{Robustness (no DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/sensor_std/inf_dist}
    \caption{$\ell_\infty$-norm (no DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/sensor_std/l2_dist}
    \caption{$\ell_2$-norm (no DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/visual_random/error}
    \caption{Robustness (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/visual_random/inf_dist}
    \caption{$\ell_\infty$-norm (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/visual_random/l2_dist}
    \caption{$\ell_2$-norm (DR, no proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/sensor_random/error}
    \caption{Robustness (DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/sensor_random/inf_dist}
    \caption{$\ell_\infty$-norm (DR, proprioception)}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/chapter6/robustness/jaco/sensor_random/l2_dist}
    \caption{$\ell_2$-norm (DR, proprioception)}
  \end{subfigure}
  \caption{Performance of re-initialisation robustness (0 for 100$\%$ success rate, and 1 for 0$\%$ success rate), and change of parameters in $\ell_\infty$- and $\ell_2$-norm of Jaco agents. The best Fetch model is selected for each training configuration.}
  %Note that the final failure rate of the best Jaco agent trained with DR and without proprioception on the standard environment is around 20\%. The re-initialisation robustness plot for this condition (g) indicates that all layers are necessary and that training continues to improve performance in the epochs depicted and beyond.
  \label{fig:jaco_robustness}
\end{figure}

\subsection{Recurrent Ablation}
We then evaluate the effectiveness of the LSTM layer for the training. In this case, the hidden states and cell states are set to constant values for the evaluation. From Table~\ref{tbl:hidden_ablation}, the LSTM layer with constant values only has a minor influence on non-DR agents, while it has a significant influence on the DR agents. This suggests that the LSTM layer is functional when DR is activated during training, otherwise it is unnecessary to learn a mapping from observations to policy. 

\begin{table}[h!]
  \centering
  \begin{tabular}{c|cc|cc}
    \toprule
    Robot & DR & Prop. & Standard & Constant Hidden\\
    \midrule
    Fetch & {\xmark} & {\xmark} & 1.000$\pm 0.000$ & 0.988$\pm 0.016$\\
    Fetch & {\xmark} & {\cmark} & 1.000$\pm 0.000$ & 0.990$\pm 0.012$\\
    Fetch & {\cmark} & {\xmark} & 0.983$\pm 0.004$ & 0.658$\pm 0.106$\\
    Fetch & {\cmark} & {\cmark} & 0.997$\pm 0.002$ & 0.838$\pm 0.051$\\
    \hline
    Jaco  & {\xmark} & {\xmark} & 0.995$\pm 0.003$ & 0.919$\pm 0.032$\\
    Jaco  & {\xmark} & {\cmark} & 0.995$\pm 0.001$ & 0.943$\pm 0.022$\\
    Jaco  & {\cmark} & {\xmark} & 0.650$\pm 0.056$ & 0.422$\pm 0.083$\\
    Jaco  & {\cmark} & {\cmark} & 0.991$\pm 0.004$ & 0.746$\pm 0.060$\\
    \bottomrule
  \end{tabular}
  \caption{Performance of recurrent ablations of all models with standard operation and constant (empirical average) hidden states. Checkmarks and crosses indicate activate/deactivate DR and proprioceptive inputs (Prop.), respectively. Mean $\pm$ standard error are calculated over all models (seeds) and test target locations.}
  \label{tbl:hidden_ablation}
\end{table}
% discussion
\section{Discussion}
This chapter aims to analyse and understand what strategies and representations the agents have learned in a simulated robot manipulation task, especially with domain randomisation (DR). In this experiment, we construct 8 different training configurations, which cover different morphologies and control schemes of the robot arm (Fetch and Jaco robot arms), different modalities of inputs (visual inputs or visual + proprioceptive inputs), and using or not using DR. We also setup 8 different test conditions to evaluate the agents' robustness to local and global visual changes and the ability of visual self-localisation. Finally, various interpretability techniques are adopted to provide both quantitative and qualitative analysis of the agents.

From the results, the agent trained with DR shows better generalisation ability to local visual changes, such as distractors with different colours and shapes. While DR agents also present a degree of robustness to global visual changes, such as illumination (direction and intensity), RGB camera translation, additive noise, and reflective surfaces, they still show significant performance degradation in several scenarios. The above failure cases can remind us to consider more factors in the DR design, where we only focus on visual DR in this chapter (e.g., texture, colours and noise). From saliency maps, the non-DR agents show more highlighted regions over the distractors, and DR agents focus more attention on the task related objects (e.g., robot arm and target ball). 

\td{In unit ablations, due to the task complexity and the control scheme of the Jaco environment, the Jaco agents require more channels for learning the representations to solve the task.} In layer re-initialisation, DR leads to distinguishable changes in $\ell_{\infty}$-norm of parameters in the convolutional layers and therefore acquires more robust features. It also shows that, when the proprioceptive inputs are available, the fc1 layer of the Jaco DR agent requires longer periods for training,  because it needs to learn a better representation from proprioceptive inputs for self-localisation. This trend is also in line with the results in the test of visual self-localisation, where this agent can still perform the task without the rendered robot arm. In this case, the agent only utilises proprioceptive inputs to infer the location of the robot arm. In the test of recurrent ablations, the LSTM layer is only necessary when DR is activated.

Finally, some recommendations are also provided to those who intend to investigate DRL agents.
\begin{itemize}
\item[1)] Do not assume that results generalise. While the Jaco agent trained with DR and without proprioception obeys some trends, it remains an outlier in many other aspects.
\item[2)] Do not expect clear results. While some methods, such as the saliency map, uncovered clear trends, others, such as unit ablations, did not. This does not mean that unit ablations are generally useless---one can imagine that if several units were highly important without DR and no units are individually important with DR, we would have found a significant trend using this method.
\item[3)] If possible, use a range of interpretability methods. Uncertainty about results from one method may be resolved by results from another method.
\item[4)] No single interpretability method is more useful than another---each type of method reveals different, complementary pieces of information.
\item[5)] Finally, use interpretability methods before making claims about the strategies used by agents. While one may assume that an agent that performs well at a task on average is using an intelligent strategy, in all likelihood, it may be using heuristics that fail to generalise \cite{ruderman2019uncovering}.
\end{itemize}

\label{ch6:discussion}